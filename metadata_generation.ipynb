{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3cd517d-31b2-4504-9af5-27989a831326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.24.3\n",
      "  Using cached numpy-1.24.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (5.6 kB)\n",
      "Using cached numpy-1.24.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (14.0 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.24.3\n",
      "Requirement already satisfied: pip in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (25.1.1)\n",
      "Requirement already satisfied: PyPDF2 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: python-docx in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: pytesseract in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (0.3.13)\n",
      "Requirement already satisfied: pillow in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (10.3.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from python-docx) (5.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from python-docx) (4.12.2)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from pytesseract) (24.2)\n",
      "Requirement already satisfied: PyMuPDF in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (1.26.1)\n",
      "Requirement already satisfied: spacy in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (3.8.6)\n",
      "Requirement already satisfied: transformers in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (4.52.4)\n",
      "Requirement already satisfied: sentence-transformers in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (4.1.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (3.1.5)\n",
      "Requirement already satisfied: setuptools in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.6.15)\n",
      "Requirement already satisfied: cloudpickle>=2.2.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from srsly<3.0.0,>=2.4.3->spacy) (3.1.1)\n",
      "Requirement already satisfied: ujson>=1.35 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from srsly<3.0.0,>=2.4.3->spacy) (5.10.0)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (63 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: wrapt in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: filelock in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: Pillow in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (14.3 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.2.6\n",
      "Requirement already satisfied: nltk in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: textstat in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (0.7.7)\n",
      "Requirement already satisfied: click in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: pyphen in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from textstat) (0.17.2)\n",
      "Requirement already satisfied: cmudict in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from textstat) (1.0.32)\n",
      "Requirement already satisfied: setuptools in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from textstat) (72.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=5 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from cmudict->textstat) (8.6.1)\n",
      "Requirement already satisfied: importlib-resources>=5 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from cmudict->textstat) (6.5.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from importlib-metadata>=5->cmudict->textstat) (3.21.0)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Requirement already satisfied: streamlit in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (1.46.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (6.1.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (8.2.1)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (2.2.6)\n",
      "Requirement already satisfied: packaging<26,>=20 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (10.3.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (6.31.1)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (20.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (4.12.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (3.1.44)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (1.43.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2023.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.23.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Requirement already satisfied: pandas in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "#STEP-1\n",
    "# First, let's handle the numpy version conflict\n",
    "!pip install numpy==1.24.3  # Compatible version\n",
    "\n",
    "# Install packages in specific order to avoid conflicts\n",
    "!pip install --upgrade pip\n",
    "\n",
    "# Document processing (install PyMuPDF with correct name)\n",
    "!pip install PyPDF2 python-docx pytesseract pillow\n",
    "!pip install PyMuPDF  # This is the correct package name, not 'fitz' or 'pymupdf'\n",
    "\n",
    "# NLP and ML packages\n",
    "!pip install spacy transformers sentence-transformers\n",
    "!pip install nltk textstat\n",
    "\n",
    "# Download spaCy model\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Web framework\n",
    "!pip install streamlit\n",
    "\n",
    "# Data handling\n",
    "!pip install pandas\n",
    "\n",
    "# OCR support (if needed)\n",
    "# Note: You'll need to install tesseract separately on your system\n",
    "# For Ubuntu/Debian: sudo apt-get install tesseract-ocr\n",
    "# For macOS: brew install tesseract\n",
    "# For Windows: Download from https://github.com/UB-Mannheim/tesseract/wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cac32ad-4cbf-4c43-861f-d10faf72b4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PyPDF2 imported successfully\n",
      "✓ python-docx imported successfully\n",
      "✓ PyMuPDF imported successfully\n",
      "✓ PIL imported successfully\n",
      "✓ pytesseract imported successfully\n",
      "✓ Tesseract OCR is available\n",
      "✓ spaCy imported and model loaded successfully\n",
      "✓ Transformers imported successfully\n",
      "✓ Sentence Transformers imported successfully\n",
      "✓ NLTK and textstat imported successfully\n",
      "\n",
      "==================================================\n",
      "Setup complete! Check above for any failed imports.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "#STEP-2\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Document processing imports with error handling\n",
    "try:\n",
    "    import PyPDF2\n",
    "    print(\"✓ PyPDF2 imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ PyPDF2 import failed: {e}\")\n",
    "\n",
    "try:\n",
    "    from docx import Document\n",
    "    print(\"✓ python-docx imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ python-docx import failed: {e}\")\n",
    "\n",
    "try:\n",
    "    import fitz  # PyMuPDF\n",
    "    print(\"✓ PyMuPDF imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ PyMuPDF import failed: {e}\")\n",
    "    print(\"Try: pip install PyMuPDF\")\n",
    "\n",
    "try:\n",
    "    from PIL import Image\n",
    "    print(\"✓ PIL imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ PIL import failed: {e}\")\n",
    "\n",
    "try:\n",
    "    import pytesseract\n",
    "    print(\"✓ pytesseract imported successfully\")\n",
    "    # Test if tesseract is installed\n",
    "    try:\n",
    "        pytesseract.get_tesseract_version()\n",
    "        print(\"✓ Tesseract OCR is available\")\n",
    "    except:\n",
    "        print(\"⚠ Tesseract OCR not found - OCR features will be disabled\")\n",
    "        pytesseract = None\n",
    "except ImportError as e:\n",
    "    print(f\"✗ pytesseract import failed: {e}\")\n",
    "    pytesseract = None\n",
    "\n",
    "# NLP imports with error handling\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"✓ spaCy imported and model loaded successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ spaCy import failed: {e}\")\n",
    "except OSError as e:\n",
    "    print(f\"✗ spaCy model not found: {e}\")\n",
    "    print(\"Run: python -m spacy download en_core_web_sm\")\n",
    "\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "    print(\"✓ Transformers imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Transformers import failed: {e}\")\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    print(\"✓ Sentence Transformers imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Sentence Transformers import failed: {e}\")\n",
    "\n",
    "try:\n",
    "    import nltk\n",
    "    from textstat import flesch_reading_ease, automated_readability_index\n",
    "    print(\"✓ NLTK and textstat imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ NLTK/textstat import failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Setup complete! Check above for any failed imports.\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be214f17-d504-4fb6-864f-db37df1c3b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-3\n",
    "def extract_pdf_text(file_path):\n",
    "    \"\"\"Extract text from PDF files with fallback to OCR\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        # Try standard PDF text extraction\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text()\n",
    "        \n",
    "        # If text is minimal, use OCR\n",
    "        if len(text.strip()) < 100:\n",
    "            text = extract_pdf_with_ocr(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting PDF: {e}\")\n",
    "        text = extract_pdf_with_ocr(file_path)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0faae6e2-2cee-4441-a395-0c42f55e30d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-4\n",
    "def extract_docx_text(file_path):\n",
    "    \"\"\"Extract text from DOCX files\"\"\"\n",
    "    try:\n",
    "        doc = Document(file_path)\n",
    "        text = \"\"\n",
    "        for paragraph in doc.paragraphs:\n",
    "            text += paragraph.text + \"\\n\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting DOCX: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd87efee-504b-43ca-98b3-976a9b7d6c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesseract version: 5.3.4\n"
     ]
    }
   ],
   "source": [
    "#STEP-5\n",
    "def extract_pdf_with_ocr(file_path):\n",
    "    \"\"\"Extract text using OCR for image-based PDFs\"\"\"\n",
    "    if pytesseract is None:\n",
    "        print(\"OCR not available - pytesseract not installed\")\n",
    "        return \"\"\n",
    "    \n",
    "    text = \"\"\n",
    "    try:\n",
    "        if 'fitz' in globals():\n",
    "            # Use PyMuPDF for OCR\n",
    "            pdf_document = fitz.open(file_path)\n",
    "            for page_num in range(pdf_document.page_count):\n",
    "                page = pdf_document[page_num]\n",
    "                pix = page.get_pixmap()\n",
    "                img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "                text += pytesseract.image_to_string(img) + \"\\n\"\n",
    "            pdf_document.close()\n",
    "        else:\n",
    "            print(\"PyMuPDF not available - OCR disabled\")\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"OCR extraction failed: {e}\")\n",
    "    return text\n",
    "\n",
    "def check_tesseract_installation():\n",
    "    \"\"\"Check if Tesseract is properly installed\"\"\"\n",
    "    try:\n",
    "        if pytesseract is not None:\n",
    "            version = pytesseract.get_tesseract_version()\n",
    "            print(f\"Tesseract version: {version}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Pytesseract not available\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Tesseract not properly installed: {e}\")\n",
    "        print(\"Please install Tesseract OCR:\")\n",
    "        print(\"- Ubuntu/Debian: sudo apt-get install tesseract-ocr\")\n",
    "        print(\"- macOS: brew install tesseract\")\n",
    "        print(\"- Windows: Download from https://github.com/UB-Mannheim/tesseract/wiki\")\n",
    "        return False\n",
    "\n",
    "# Check OCR availability\n",
    "ocr_available = check_tesseract_installation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2a44db5-c4d0-4126-9942-138b0e4763b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ spaCy model initialized\n",
      "✓ Sentence Transformer model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Summarization model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Classification model loaded\n"
     ]
    }
   ],
   "source": [
    "#STEP-6\n",
    "# Initialize models with error handling\n",
    "def initialize_models():\n",
    "    \"\"\"Initialize NLP models with fallback options\"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    # Initialize spaCy model\n",
    "    try:\n",
    "        if 'nlp' in globals():\n",
    "            models['nlp'] = nlp\n",
    "            print(\"✓ spaCy model initialized\")\n",
    "        else:\n",
    "            print(\"✗ spaCy not available\")\n",
    "            models['nlp'] = None\n",
    "    except Exception as e:\n",
    "        print(f\"✗ spaCy initialization failed: {e}\")\n",
    "        models['nlp'] = None\n",
    "    \n",
    "    # Initialize sentence transformer\n",
    "    try:\n",
    "        models['sentence_model'] = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        print(\"✓ Sentence Transformer model loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Sentence Transformer failed: {e}\")\n",
    "        models['sentence_model'] = None\n",
    "    \n",
    "    # Initialize summarizer\n",
    "    try:\n",
    "        models['summarizer'] = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "        print(\"✓ Summarization model loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Summarization model failed: {e}\")\n",
    "        models['summarizer'] = None\n",
    "    \n",
    "    # Initialize classifier\n",
    "    try:\n",
    "        models['classifier'] = pipeline(\"text-classification\", \n",
    "                                       model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "        print(\"✓ Classification model loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Classification model failed: {e}\")\n",
    "        models['classifier'] = None\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Initialize all models\n",
    "models = initialize_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e508b1d8-11c0-45d9-9aba-42a0a91dccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-7\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess extracted text\"\"\"\n",
    "    # Remove extra whitespace and normalize\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Remove special characters but keep basic punctuation\n",
    "    text = re.sub(r'[^\\w\\s.,!?;:-]', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d0b631e-031b-4f01-8e82-98f20a3c5799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-8\n",
    "def extract_key_information(text):\n",
    "    \"\"\"Extract entities, keywords, and important sections\"\"\"\n",
    "    entities = {\n",
    "        'PERSON': [],\n",
    "        'ORG': [],\n",
    "        'GPE': [],  # Geopolitical entities\n",
    "        'DATE': [],\n",
    "        'MONEY': [],\n",
    "        'PERCENT': []\n",
    "    }\n",
    "    key_phrases = []\n",
    "    \n",
    "    try:\n",
    "        if models['nlp'] is not None:\n",
    "            doc = models['nlp'](text)\n",
    "            \n",
    "            # Extract named entities\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ in entities:\n",
    "                    entities[ent.label_].append(ent.text)\n",
    "            \n",
    "            # Extract key phrases using noun chunks\n",
    "            key_phrases = [chunk.text for chunk in doc.noun_chunks if len(chunk.text.split()) > 1]\n",
    "        else:\n",
    "            # Fallback: simple regex-based extraction\n",
    "            print(\"Using fallback entity extraction\")\n",
    "            # Simple patterns for common entities\n",
    "            import re\n",
    "            \n",
    "            # Extract potential names (capitalized words)\n",
    "            names = re.findall(r'\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\b', text)\n",
    "            entities['PERSON'] = list(set(names))[:10]  # Top 10 unique names\n",
    "            \n",
    "            # Extract dates\n",
    "            dates = re.findall(r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b|\\b\\d{4}\\b', text)\n",
    "            entities['DATE'] = list(set(dates))[:5]\n",
    "            \n",
    "            # Extract percentages\n",
    "            percentages = re.findall(r'\\b\\d+(?:\\.\\d+)?%\\b', text)\n",
    "            entities['PERCENT'] = list(set(percentages))\n",
    "            \n",
    "            # Simple key phrase extraction (common noun phrases)\n",
    "            words = text.split()\n",
    "            bigrams = [f\"{words[i]} {words[i+1]}\" for i in range(len(words)-1)]\n",
    "            key_phrases = list(set(bigrams))[:10]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Entity extraction failed: {e}\")\n",
    "    \n",
    "    return entities, key_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f6d9f46-1be9-467a-aa6d-e7a4f330dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-9\n",
    "def classify_document_type(text):\n",
    "    \"\"\"Classify document type based on content\"\"\"\n",
    "    # Simple rule-based classification\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    if any(word in text_lower for word in ['contract', 'agreement', 'terms', 'legal']):\n",
    "        return 'Legal Document'\n",
    "    elif any(word in text_lower for word in ['report', 'analysis', 'findings', 'conclusion']):\n",
    "        return 'Report'\n",
    "    elif any(word in text_lower for word in ['manual', 'guide', 'instructions', 'how to']):\n",
    "        return 'Manual/Guide'\n",
    "    elif any(word in text_lower for word in ['proposal', 'project', 'plan', 'strategy']):\n",
    "        return 'Proposal/Plan'\n",
    "    else:\n",
    "        return 'General Document'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63183160-80ec-4d23-a9bc-dfecce10ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-10\n",
    "def generate_summary(text, max_length=150):\n",
    "    \"\"\"Generate document summary\"\"\"\n",
    "    try:\n",
    "        if models['summarizer'] is not None:\n",
    "            # Chunk text if too long\n",
    "            if len(text) > 1024:\n",
    "                chunks = [text[i:i+1024] for i in range(0, len(text), 1024)]\n",
    "                summaries = []\n",
    "                for chunk in chunks[:3]:  # Process first 3 chunks\n",
    "                    if len(chunk.strip()) > 50:  # Only process substantial chunks\n",
    "                        summary = models['summarizer'](chunk, max_length=50, min_length=10, do_sample=False)\n",
    "                        summaries.append(summary[0]['summary_text'])\n",
    "                return ' '.join(summaries)\n",
    "            else:\n",
    "                if len(text.strip()) > 50:\n",
    "                    summary = models['summarizer'](text, max_length=max_length, min_length=30, do_sample=False)\n",
    "                    return summary[0]['summary_text']\n",
    "                else:\n",
    "                    return text[:max_length] + \"...\" if len(text) > max_length else text\n",
    "        else:\n",
    "            # Fallback: extract first few sentences\n",
    "            sentences = text.split('.')\n",
    "            summary_sentences = []\n",
    "            char_count = 0\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                if char_count + len(sentence) < max_length:\n",
    "                    summary_sentences.append(sentence.strip())\n",
    "                    char_count += len(sentence)\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            return '. '.join(summary_sentences[:3]) + '.' if summary_sentences else text[:max_length]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Summarization failed: {e}\")\n",
    "        # Final fallback\n",
    "        sentences = text.split('.')[:3]\n",
    "        return '. '.join([s.strip() for s in sentences if s.strip()]) + '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67d51ad4-943c-4273-83c5-0d10e5fc2409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-11\n",
    "def extract_topics(text, num_topics=5):\n",
    "    \"\"\"Extract main topics from document\"\"\"\n",
    "    try:\n",
    "        if models['nlp'] is not None:\n",
    "            doc = models['nlp'](text)\n",
    "            \n",
    "            # Get important tokens (nouns, adjectives)\n",
    "            important_tokens = [token.lemma_.lower() for token in doc \n",
    "                               if token.pos_ in ['NOUN', 'ADJ'] and len(token.text) > 3]\n",
    "        else:\n",
    "            # Fallback: simple word frequency\n",
    "            import re\n",
    "            words = re.findall(r'\\b[a-zA-Z]{4,}\\b', text.lower())\n",
    "            # Filter out common words\n",
    "            stop_words = {'this', 'that', 'with', 'have', 'will', 'from', 'they', 'been', \n",
    "                         'have', 'were', 'said', 'each', 'which', 'their', 'time', 'but'}\n",
    "            important_tokens = [word for word in words if word not in stop_words]\n",
    "        \n",
    "        # Count frequency\n",
    "        word_freq = Counter(important_tokens)\n",
    "        \n",
    "        # Return top topics\n",
    "        return [word for word, count in word_freq.most_common(num_topics)]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Topic extraction failed: {e}\")\n",
    "        # Simple fallback\n",
    "        words = text.lower().split()\n",
    "        word_freq = Counter(word for word in words if len(word) > 4)\n",
    "        return [word for word, count in word_freq.most_common(num_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e19c044d-9607-493c-bc1a-22c80e1a8492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-12\n",
    "def create_metadata_schema():\n",
    "    \"\"\"Define the metadata structure\"\"\"\n",
    "    return {\n",
    "        'basic_info': {\n",
    "            'filename': '',\n",
    "            'file_type': '',\n",
    "            'file_size': 0,\n",
    "            'creation_date': '',\n",
    "            'processing_date': ''\n",
    "        },\n",
    "        'content_analysis': {\n",
    "            'document_type': '',\n",
    "            'language': 'en',\n",
    "            'word_count': 0,\n",
    "            'character_count': 0,\n",
    "            'readability_score': 0,\n",
    "            'sentiment': ''\n",
    "        },\n",
    "        'semantic_data': {\n",
    "            'summary': '',\n",
    "            'key_topics': [],\n",
    "            'entities': {},\n",
    "            'key_phrases': []\n",
    "        },\n",
    "        'technical_metadata': {\n",
    "            'extraction_method': '',\n",
    "            'confidence_score': 0.0,\n",
    "            'processing_time': 0.0\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d971030f-51ce-4493-82d2-991b05980212",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-13\n",
    "def generate_metadata(file_path):\n",
    "    \"\"\"Main function to generate complete metadata\"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize metadata structure\n",
    "    metadata = create_metadata_schema()\n",
    "    \n",
    "    # Extract basic file information\n",
    "    file_info = Path(file_path)\n",
    "    metadata['basic_info']['filename'] = file_info.name\n",
    "    metadata['basic_info']['file_type'] = file_info.suffix\n",
    "    metadata['basic_info']['file_size'] = file_info.stat().st_size\n",
    "    metadata['basic_info']['processing_date'] = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Extract text content\n",
    "    if file_path.endswith('.pdf'):\n",
    "        text = extract_pdf_text(file_path)\n",
    "        extraction_method = 'PDF extraction'\n",
    "    elif file_path.endswith('.docx'):\n",
    "        text = extract_docx_text(file_path)\n",
    "        extraction_method = 'DOCX extraction'\n",
    "    else:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        extraction_method = 'Direct text reading'\n",
    "    \n",
    "    # Preprocess text\n",
    "    text = preprocess_text(text)\n",
    "    \n",
    "    # Content analysis\n",
    "    metadata['content_analysis']['word_count'] = len(text.split())\n",
    "    metadata['content_analysis']['character_count'] = len(text)\n",
    "    metadata['content_analysis']['document_type'] = classify_document_type(text)\n",
    "    metadata['content_analysis']['readability_score'] = flesch_reading_ease(text)\n",
    "    \n",
    "    # Semantic analysis\n",
    "    metadata['semantic_data']['summary'] = generate_summary(text)\n",
    "    metadata['semantic_data']['key_topics'] = extract_topics(text)\n",
    "    entities, key_phrases = extract_key_information(text)\n",
    "    metadata['semantic_data']['entities'] = entities\n",
    "    metadata['semantic_data']['key_phrases'] = key_phrases[:10]  # Top 10 phrases\n",
    "    \n",
    "    # Technical metadata\n",
    "    metadata['technical_metadata']['extraction_method'] = extraction_method\n",
    "    metadata['technical_metadata']['processing_time'] = time.time() - start_time\n",
    "    metadata['technical_metadata']['confidence_score'] = calculate_confidence_score(text)\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98b00fb4-2537-461e-a0dc-4ed762b945f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-14\n",
    "import streamlit as st\n",
    "\n",
    "def create_streamlit_interface():\n",
    "    \"\"\"Create web interface using Streamlit\"\"\"\n",
    "    st.title(\"Automated Metadata Generation System\")\n",
    "    st.write(\"Upload documents to generate comprehensive metadata\")\n",
    "    \n",
    "    # File upload\n",
    "    uploaded_file = st.file_uploader(\n",
    "        \"Choose a file\", \n",
    "        type=['pdf', 'docx', 'txt'],\n",
    "        help=\"Upload PDF, DOCX, or TXT files\"\n",
    "    )\n",
    "    \n",
    "    if uploaded_file is not None:\n",
    "        # Save uploaded file temporarily\n",
    "        with open(f\"temp_{uploaded_file.name}\", \"wb\") as f:\n",
    "            f.write(uploaded_file.getbuffer())\n",
    "        \n",
    "        # Generate metadata\n",
    "        with st.spinner('Generating metadata...'):\n",
    "            metadata = generate_metadata(f\"temp_{uploaded_file.name}\")\n",
    "        \n",
    "        # Display results\n",
    "        display_metadata(metadata)\n",
    "        \n",
    "        # Clean up\n",
    "        os.remove(f\"temp_{uploaded_file.name}\")\n",
    "\n",
    "def display_metadata(metadata):\n",
    "    \"\"\"Display metadata in organized tabs\"\"\"\n",
    "    tab1, tab2, tab3, tab4 = st.tabs([\"Basic Info\", \"Content Analysis\", \"Semantic Data\", \"Technical Info\"])\n",
    "    \n",
    "    with tab1:\n",
    "        st.json(metadata['basic_info'])\n",
    "    \n",
    "    with tab2:\n",
    "        st.json(metadata['content_analysis'])\n",
    "    \n",
    "    with tab3:\n",
    "        st.write(\"**Summary:**\")\n",
    "        st.write(metadata['semantic_data']['summary'])\n",
    "        st.write(\"**Key Topics:**\")\n",
    "        st.write(metadata['semantic_data']['key_topics'])\n",
    "        st.write(\"**Entities:**\")\n",
    "        st.json(metadata['semantic_data']['entities'])\n",
    "    \n",
    "    with tab4:\n",
    "        st.json(metadata['technical_metadata'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7113e156-48dc-4498-aaab-ab4ff2ca0133",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-15\n",
    "def process_multiple_documents(folder_path):\n",
    "    \"\"\"Process multiple documents in a folder\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for file_path in Path(folder_path).glob('*'):\n",
    "        if file_path.suffix in ['.pdf', '.docx', '.txt']:\n",
    "            try:\n",
    "                metadata = generate_metadata(str(file_path))\n",
    "                results.append(metadata)\n",
    "                print(f\"Processed: {file_path.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path.name}: {e}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98338327-fda4-487e-bdfd-ebd1db518382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-16\n",
    "def save_metadata_to_json(metadata, output_path):\n",
    "    \"\"\"Save metadata to JSON file\"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def export_to_csv(metadata_list, output_path):\n",
    "    \"\"\"Export metadata list to CSV\"\"\"\n",
    "    # Flatten metadata for CSV export\n",
    "    flattened_data = []\n",
    "    for metadata in metadata_list:\n",
    "        flat_dict = {}\n",
    "        for category, data in metadata.items():\n",
    "            if isinstance(data, dict):\n",
    "                for key, value in data.items():\n",
    "                    flat_dict[f\"{category}_{key}\"] = str(value)\n",
    "            else:\n",
    "                flat_dict[category] = str(data)\n",
    "        flattened_data.append(flat_dict)\n",
    "    \n",
    "    df = pd.DataFrame(flattened_data)\n",
    "    df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c5fb2-cff8-4d84-beae-41e5595b5c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated Metadata Generation System\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose startup mode:\n",
      "1: Full system (may take time to load)\n",
      "2: Quick start (basic features only)\n",
      "Enter choice (1 or 2):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated Metadata Generation System\n",
      "========================================\n",
      "Checking system readiness...\n",
      "Models loaded: 3/3\n",
      "\n",
      "Select operation mode:\n",
      "1: Process single file\n",
      "2: Batch process folder\n",
      "3: Start web interface\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter choice (1-3):  2\n",
      "Enter folder path:  /home/navya/Downloads/test_metadata/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing documents in batch...\n"
     ]
    }
   ],
   "source": [
    "#STEP-17\n",
    "# Required imports for main execution\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function with progress indicators\"\"\"\n",
    "    print(\"Automated Metadata Generation System\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Check if models are already loaded\n",
    "    print(\"Checking system readiness...\")\n",
    "    models_ready = check_models_status()\n",
    "    \n",
    "    if not models_ready:\n",
    "        print(\"⚠ Some models not loaded. System will use fallback methods.\")\n",
    "        load_choice = input(\"Load ML models now? (y/n) [This may take a few minutes]: \")\n",
    "        if load_choice.lower() == 'y':\n",
    "            print(\"Loading models... This may take a few minutes on first run.\")\n",
    "            models = initialize_models()\n",
    "    \n",
    "    # Choose mode\n",
    "    print(\"\\nSelect operation mode:\")\n",
    "    print(\"1: Process single file\")\n",
    "    print(\"2: Batch process folder\")\n",
    "    print(\"3: Start web interface\")\n",
    "    \n",
    "    mode = input(\"Enter choice (1-3): \")\n",
    "    \n",
    "    if mode == \"1\":\n",
    "        file_path = input(\"Enter file path: \")\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            return\n",
    "        \n",
    "        print(\"Processing file... Please wait.\")\n",
    "        start_time = time.time()\n",
    "        metadata = generate_metadata(file_path)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"\\n✓ Processing completed in {end_time - start_time:.2f} seconds\")\n",
    "        print(json.dumps(metadata, indent=2))\n",
    "        \n",
    "        # Save option\n",
    "        save_option = input(\"\\nSave to file? (y/n): \")\n",
    "        if save_option.lower() == 'y':\n",
    "            output_path = input(\"Enter output path: \")\n",
    "            save_metadata_to_json(metadata, output_path)\n",
    "            print(f\"Metadata saved to: {output_path}\")\n",
    "    \n",
    "    elif mode == \"2\":\n",
    "        folder_path = input(\"Enter folder path: \")\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Folder not found: {folder_path}\")\n",
    "            return\n",
    "            \n",
    "        print(\"Processing documents in batch...\")\n",
    "        results = process_multiple_documents(folder_path)\n",
    "        print(f\"✓ Processed {len(results)} documents\")\n",
    "        \n",
    "        # Export option\n",
    "        export_option = input(\"Export to CSV? (y/n): \")\n",
    "        if export_option.lower() == 'y':\n",
    "            output_path = input(\"Enter CSV output path: \")\n",
    "            export_to_csv(results, output_path)\n",
    "            print(f\"Results exported to: {output_path}\")\n",
    "    \n",
    "    elif mode == \"3\":\n",
    "        print(\"Starting web interface...\")\n",
    "        print(\"This will open in your browser. Press Ctrl+C to stop.\")\n",
    "        try:\n",
    "            create_streamlit_interface()\n",
    "        except Exception as e:\n",
    "            print(f\"Web interface failed: {e}\")\n",
    "            print(\"Make sure Streamlit is installed: pip install streamlit\")\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        print(\"Invalid choice. Please run again and select 1-3.\")\n",
    "\n",
    "def check_models_status():\n",
    "    \"\"\"Quick check of model availability\"\"\"\n",
    "    try:\n",
    "        status = {\n",
    "            'nlp': models.get('nlp') is not None,\n",
    "            'summarizer': models.get('summarizer') is not None,\n",
    "            'sentence_model': models.get('sentence_model') is not None\n",
    "        }\n",
    "        \n",
    "        loaded_count = sum(status.values())\n",
    "        total_count = len(status)\n",
    "        \n",
    "        print(f\"Models loaded: {loaded_count}/{total_count}\")\n",
    "        return loaded_count > 0\n",
    "    except NameError:\n",
    "        print(\"Models not initialized yet.\")\n",
    "        return False\n",
    "\n",
    "# Fast startup version - minimal model loading\n",
    "def quick_start():\n",
    "    \"\"\"Quick start with minimal dependencies\"\"\"\n",
    "    print(\"🚀 Quick Start Mode - Basic metadata extraction only\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    file_path = input(\"Enter file path: \").strip().strip('\"\\'')  # Remove quotes if present\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        print(\"Please check the file path and try again.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing: {file_path}\")\n",
    "    \n",
    "    # Basic metadata without heavy ML models\n",
    "    try:\n",
    "        metadata = generate_basic_metadata(file_path)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"METADATA GENERATED:\")\n",
    "        print(\"=\"*50)\n",
    "        print(json.dumps(metadata, indent=2))\n",
    "        \n",
    "        # Save option\n",
    "        save_option = input(\"\\nSave to JSON file? (y/n): \")\n",
    "        if save_option.lower() == 'y':\n",
    "            output_path = input(\"Enter output filename (e.g., metadata.json): \")\n",
    "            if not output_path.endswith('.json'):\n",
    "                output_path += '.json'\n",
    "            save_metadata_to_json(metadata, output_path)\n",
    "            print(f\"✓ Metadata saved to: {output_path}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "def generate_basic_metadata(file_path):\n",
    "    \"\"\"Generate basic metadata without ML models\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize basic metadata structure\n",
    "    metadata = create_metadata_schema()\n",
    "    \n",
    "    # Extract basic file information\n",
    "    file_info = Path(file_path)\n",
    "    metadata['basic_info']['filename'] = file_info.name\n",
    "    metadata['basic_info']['file_type'] = file_info.suffix\n",
    "    metadata['basic_info']['file_size'] = file_info.stat().st_size\n",
    "    metadata['basic_info']['creation_date'] = time.ctime(file_info.stat().st_ctime)\n",
    "    metadata['basic_info']['processing_date'] = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Extract text content\n",
    "    text = \"\"\n",
    "    extraction_method = \"\"\n",
    "    \n",
    "    try:\n",
    "        if file_path.lower().endswith('.pdf'):\n",
    "            text = extract_pdf_text(file_path)\n",
    "            extraction_method = 'PDF extraction'\n",
    "        elif file_path.lower().endswith('.docx'):\n",
    "            text = extract_docx_text(file_path)\n",
    "            extraction_method = 'DOCX extraction'\n",
    "        elif file_path.lower().endswith(('.txt', '.md')):\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                text = f.read()\n",
    "            extraction_method = 'Direct text reading'\n",
    "        else:\n",
    "            # Try to read as text anyway\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                text = f.read()\n",
    "            extraction_method = 'Generic text reading'\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Text extraction failed: {e}\")\n",
    "        text = f\"[Text extraction failed: {str(e)}]\"\n",
    "        extraction_method = 'Failed extraction'\n",
    "    \n",
    "    # Preprocess text\n",
    "    text = preprocess_text(text) if text else \"\"\n",
    "    \n",
    "    # Basic content analysis\n",
    "    words = text.split() if text else []\n",
    "    metadata['content_analysis']['word_count'] = len(words)\n",
    "    metadata['content_analysis']['character_count'] = len(text)\n",
    "    metadata['content_analysis']['document_type'] = classify_document_type(text)\n",
    "    \n",
    "    # Try to calculate readability score\n",
    "    try:\n",
    "        from textstat import flesch_reading_ease\n",
    "        metadata['content_analysis']['readability_score'] = flesch_reading_ease(text) if text else 0\n",
    "    except:\n",
    "        metadata['content_analysis']['readability_score'] = 0\n",
    "    \n",
    "    # Simple summary (first 200 characters)\n",
    "    if text:\n",
    "        sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "        summary_text = '. '.join(sentences[:2]) + '.' if sentences else text[:200]\n",
    "        metadata['semantic_data']['summary'] = summary_text + \"...\" if len(summary_text) > 200 else summary_text\n",
    "    else:\n",
    "        metadata['semantic_data']['summary'] = \"No text content extracted\"\n",
    "    \n",
    "    # Basic topics (most frequent meaningful words)\n",
    "    if text:\n",
    "        # Simple stop words to filter out\n",
    "        stop_words = {'the', 'and', 'are', 'for', 'with', 'this', 'that', 'from', 'they', 'have', 'been', 'will', 'said', 'each', 'which', 'their', 'time', 'but', 'all', 'can', 'may', 'was', 'were', 'not', 'you', 'your'}\n",
    "        words_clean = [word.lower().strip('.,!?;:\"()[]') for word in words if len(word) > 3 and word.lower() not in stop_words]\n",
    "        word_freq = Counter(words_clean)\n",
    "        metadata['semantic_data']['key_topics'] = [word for word, count in word_freq.most_common(5)]\n",
    "    else:\n",
    "        metadata['semantic_data']['key_topics'] = []\n",
    "    \n",
    "    # Basic entities (simple pattern matching)\n",
    "    entities = {'PERSON': [], 'ORG': [], 'DATE': [], 'PERCENT': []}\n",
    "    if text:\n",
    "        import re\n",
    "        # Find capitalized words (potential names/organizations)\n",
    "        capitalized = re.findall(r'\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\b', text)\n",
    "        entities['PERSON'] = list(set(capitalized))[:5]  # Top 5 unique\n",
    "        \n",
    "        # Find dates\n",
    "        dates = re.findall(r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b|\\b\\d{4}\\b', text)\n",
    "        entities['DATE'] = list(set(dates))[:3]\n",
    "        \n",
    "        # Find percentages\n",
    "        percentages = re.findall(r'\\b\\d+(?:\\.\\d+)?%\\b', text)\n",
    "        entities['PERCENT'] = list(set(percentages))\n",
    "    \n",
    "    metadata['semantic_data']['entities'] = entities\n",
    "    \n",
    "    # Technical metadata\n",
    "    metadata['technical_metadata']['extraction_method'] = extraction_method\n",
    "    metadata['technical_metadata']['processing_time'] = time.time() - start_time\n",
    "    metadata['technical_metadata']['confidence_score'] = 0.8 if text else 0.1\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "# Run the system\n",
    "if __name__ == \"__main__\":\n",
    "    # Choose startup mode\n",
    "    print(\"Automated Metadata Generation System\")\n",
    "    print(\"=\" * 50)\n",
    "    startup_mode = input(\"Choose startup mode:\\n1: Full system (may take time to load)\\n2: Quick start (basic features only)\\nEnter choice (1 or 2): \")\n",
    "    \n",
    "    if startup_mode == \"2\":\n",
    "        quick_start()\n",
    "    else:\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3ce962b-12cd-42d5-9cf0-40382ac9e41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confidence_score(text):\n",
    "    \"\"\"Calculate confidence score for metadata quality\"\"\"\n",
    "    score = 0.0\n",
    "    \n",
    "    # Text length factor\n",
    "    if len(text) > 100:\n",
    "        score += 0.3\n",
    "    \n",
    "    # Entity detection factor\n",
    "    doc = nlp(text)\n",
    "    if len(doc.ents) > 0:\n",
    "        score += 0.3\n",
    "    \n",
    "    # Structure factor (presence of sentences)\n",
    "    sentences = text.split('.')\n",
    "    if len(sentences) > 3:\n",
    "        score += 0.2\n",
    "    \n",
    "    # Readability factor\n",
    "    readability = flesch_reading_ease(text)\n",
    "    if readability > 30:  # Readable text\n",
    "        score += 0.2\n",
    "    \n",
    "    return min(score, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9cd5125-44e8-4f69-845f-416e65b5f584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Setup logging configuration\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler('metadata_generation.log'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "logger = setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2b8a78-3b74-4a7b-9ad6-fe672a50dd92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_env)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
